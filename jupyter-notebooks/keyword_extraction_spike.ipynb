{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info http://www.nltk.org/nltk_data/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# ToDo: Load only needed packages\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text1 = \"\"\"\n",
    "Text mining is the discovery and extraction of interesting, non-trivial knowledge\n",
    "from free or unstructured text. This encompasses everything from information\n",
    "retrieval (i.e., document or web site retrieval) to text classification\n",
    "and clustering, to (somewhat more recently) entity, relation, and event extraction.\n",
    "Natural language processing (NLP), is the attempt to extract a fuller\n",
    "meaning representation from free text. This can be put roughly as figuring\n",
    "out who did what to whom, when, where, how and why. NLP typically makes\n",
    "use of linguistic concepts such as part-of-speech (noun, verb, adjective, etc.)\n",
    "and grammatical structure (either represented as phrases like noun phrase\n",
    "or prepositional phrase, or dependency relations like subject-of or object-of).\n",
    "It has to deal with anaphora (what previous noun does a pronoun or other\n",
    "back-referring phrase correspond to) and ambiguities (both of words and of\n",
    "grammatical structure, such as what is being modified by a given word or\n",
    "prepositional phrase). To do this, it makes use of various knowledge representations,\n",
    "such as a lexicon of words and their meanings and grammatical\n",
    "properties and a set of grammar rules and often other resources such as an\n",
    "ontology of entities and actions, or a thesaurus of synonyms or abbreviations.\n",
    "This book has several purposes. First, we want to explore the use of NLP\n",
    "techniques in text mining, as well as some other technologies that are novel to\n",
    "the field of text mining. Second, we wish to explore novel ways of integrating\n",
    "various technologies, old or new, to solve a text mining problem. Next, we\n",
    "would like to look at some new applications for text mining. Finally, we have\n",
    "several chapters that provide various supporting techniques for either text\n",
    "mining or NLP or both, or enhancements to existing techniques.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text2 = \"\"\"\n",
    "THE NEED for a thorough textbook for Statistical Natural Language Processing\n",
    "hardly needs to be argued for in the age of on-line information,\n",
    "electronic communication and the World Wide Web. Increasingly, businesses,\n",
    "government agencies and individuals are confronted with large\n",
    "amounts of text that are critical for working and living, but not well\n",
    "enough understood to get the enormous value out of them that they potentially\n",
    "hide.\n",
    "At the same time, the availability of large text corpora has changed\n",
    "the scientific approach to language in linguistics and cognitive science.\n",
    "Phenomena that were not detectable or seemed uninteresting in studying\n",
    "toy domains and individual sentences have moved into the center field of\n",
    "what is considered important to explain. Whereas as recently as the early\n",
    "1990s quantitative methods were seen as so inadequate for linguistics\n",
    "that an important textbook for mathematical linguistics did not cover\n",
    "them in any way, they are now increasingly seen as crucial for linguistic\n",
    "theory.\n",
    "In this book we have tried to achieve a balance between theory and\n",
    "practice, and between intuition and rigor. We attempt to ground approaches\n",
    "in theoretical ideas, both mathematical and linguistic, but simultaneously\n",
    "we try to not let the material get too dry, and try to show\n",
    "how theoretical ideas have been used to solve practical problems. To do\n",
    "this, we first present key concepts in probability theory, statistics, information\n",
    "theory, and linguistics in order to give students the foundations\n",
    "to understand the field and contribute to it.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text3 = \"\"\"\n",
    "The recently introduced continuous Skip-gram model is an efficient method for\n",
    "learning high-quality distributed vector representations that capture a large number\n",
    "of precise syntactic and semantic word relationships. In this paper we present\n",
    "several extensions that improve both the quality of the vectors and the training\n",
    "speed. By subsampling of the frequent words we obtain significant speedup and\n",
    "also learn more regular word representations. We also describe a simple alternative\n",
    "to the hierarchical softmax called negative sampling.\n",
    "An inherent limitation of word representations is their indifference to word order\n",
    "and their inability to represent idiomatic phrases. For example, the meanings of\n",
    "“Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated\n",
    "by this example, we present a simple method for finding phrases in text, and show\n",
    "that learning good vector representations for millions of phrases is possible.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text4 = \"\"\"\n",
    "Knowledge Discovery in Databases (KDD) is an automatic, exploratory\n",
    "analysis and modeling of large data repositories. KDD is the organized process\n",
    "of identifying valid, novel, useful, and understandable patterns from large and\n",
    "complex data sets. Data Mining (DM) is the core of the KDD process, involving\n",
    "the inferring of algorithms that explore the data, develop the model and\n",
    "discover previously unknown patterns. The model is used for understanding\n",
    "phenomena from the data, analysis and prediction.\n",
    "The accessibility and abundance of data today makes knowledge discovery\n",
    "and Data Mining a matter of considerable importance and necessity. Given the\n",
    "recent growth of the field, it is not surprising that a wide variety of methods is\n",
    "now available to the researchers and practitioners. No one method is superior to\n",
    "others for all cases. The handbook of Data Mining and Knowledge Discovery\n",
    "from Data aims to organize all significant methods developed in the field into a\n",
    "coherent and unified catalog; presents performance evaluation approaches and\n",
    "techniques; and explains with cases and software tools the use of the different\n",
    "methods.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text5 = \"\"\"\n",
    "In view of the tremendous production of computer data worldwide, there is a strong need for new\n",
    "powerful tools that can automatically generate useful knowledge from a variety of data, and\n",
    "present it in human-oriented forms. In efforts to satisfy this need, researchers have been exploring\n",
    "ideas and methods developed in machine learning, statistical data analysis, data mining, text\n",
    "mining, data visualization, pattern recognition, etc. The first part of this chapter is a compendium\n",
    "of ideas on the applicability of symbolic machine learning and logical data analysis methods\n",
    "toward this goal. The second part outlines a multistrategy methodology for an emerging research\n",
    "direction, called knowledge mining, by which we mean the derivation of high-level concepts and\n",
    "descriptions from data through symbolic reasoning involving both data and relevant background\n",
    "knowledge. The effecbtive use of background as well as previously created knowledge in\n",
    "reasoning about new data makes it possible for the knowledge mining system to derive useful\n",
    "new knowledge not only from large amounts of data, but also from limited and weakly relevant\n",
    "data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = [text1, text2, text3, text4, text5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from functools import reduce\n",
    "\n",
    "def extract_possible_keywords(text):\n",
    "    good_tags = {'JJ','JJR','JJS','NN','NNP','NNS','NNPS'}\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english')).union(string.punctuation)\n",
    "    \n",
    "    tagged_words = reduce(lambda x,y: x + y, (nltk.pos_tag_sents(nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(text))))\n",
    "    return [word.lower() for word, tag in tagged_words\n",
    "                  if tag in good_tags and word.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text',\n",
       " 'mining',\n",
       " 'discovery',\n",
       " 'extraction',\n",
       " 'interesting',\n",
       " 'non-trivial',\n",
       " 'knowledge',\n",
       " 'free',\n",
       " 'unstructured',\n",
       " 'text',\n",
       " 'everything',\n",
       " 'information',\n",
       " 'retrieval',\n",
       " 'document',\n",
       " 'web',\n",
       " 'site',\n",
       " 'retrieval',\n",
       " 'classification',\n",
       " 'clustering',\n",
       " 'entity',\n",
       " 'relation',\n",
       " 'event',\n",
       " 'extraction',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'nlp',\n",
       " 'attempt',\n",
       " 'fuller',\n",
       " 'representation',\n",
       " 'free',\n",
       " 'text',\n",
       " 'nlp',\n",
       " 'use',\n",
       " 'linguistic',\n",
       " 'concepts',\n",
       " 'part-of-speech',\n",
       " 'noun',\n",
       " 'verb',\n",
       " 'adjective',\n",
       " 'grammatical',\n",
       " 'structure',\n",
       " 'phrases',\n",
       " 'noun',\n",
       " 'phrase',\n",
       " 'prepositional',\n",
       " 'phrase',\n",
       " 'dependency',\n",
       " 'relations',\n",
       " 'subject-of',\n",
       " 'object-of',\n",
       " 'anaphora',\n",
       " 'previous',\n",
       " 'noun',\n",
       " 'pronoun',\n",
       " 'back-referring',\n",
       " 'phrase',\n",
       " 'correspond',\n",
       " 'ambiguities',\n",
       " 'words',\n",
       " 'grammatical',\n",
       " 'structure',\n",
       " 'word',\n",
       " 'prepositional',\n",
       " 'phrase',\n",
       " 'use',\n",
       " 'various',\n",
       " 'knowledge',\n",
       " 'representations',\n",
       " 'lexicon',\n",
       " 'words',\n",
       " 'meanings',\n",
       " 'grammatical',\n",
       " 'properties',\n",
       " 'set',\n",
       " 'grammar',\n",
       " 'rules',\n",
       " 'resources',\n",
       " 'ontology',\n",
       " 'entities',\n",
       " 'actions',\n",
       " 'thesaurus',\n",
       " 'synonyms',\n",
       " 'abbreviations',\n",
       " 'book',\n",
       " 'several',\n",
       " 'purposes',\n",
       " 'use',\n",
       " 'nlp',\n",
       " 'techniques',\n",
       " 'text',\n",
       " 'mining',\n",
       " 'technologies',\n",
       " 'novel',\n",
       " 'field',\n",
       " 'text',\n",
       " 'mining',\n",
       " 'second',\n",
       " 'novel',\n",
       " 'ways',\n",
       " 'various',\n",
       " 'technologies',\n",
       " 'old',\n",
       " 'new',\n",
       " 'text',\n",
       " 'mining',\n",
       " 'problem',\n",
       " 'next',\n",
       " 'new',\n",
       " 'applications',\n",
       " 'text',\n",
       " 'mining',\n",
       " 'several',\n",
       " 'chapters',\n",
       " 'various',\n",
       " 'techniques',\n",
       " 'text',\n",
       " 'mining',\n",
       " 'nlp',\n",
       " 'enhancements',\n",
       " 'techniques']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_possible_keywords(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "def evaluate_possible_keywords(texts):\n",
    "    boc_texts = [extract_possible_keywords(text) for text in texts]\n",
    "    dictionary = gensim.corpora.Dictionary(boc_texts)\n",
    "    corpus = [dictionary.doc2bow(boc_text) for boc_text in boc_texts]\n",
    "    \n",
    "    tfidf = gensim.models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    \n",
    "    return corpus_tfidf, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_matrix, word_mapping = evaluate_possible_keywords(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['concepts', 'field', 'word', 'processing', 'web']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "[word_mapping[word_id] for word_id, tfidf_value in sorted(tfidf_matrix[0], key=itemgetter(1))[:5]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
